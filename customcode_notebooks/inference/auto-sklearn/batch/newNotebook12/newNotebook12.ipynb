{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Test cell\n",
    "\n",
    "# The variables below have to be included in the test cell in order to query the cube.\n",
    "from init_sisense import sisense_conn\n",
    "cube_name = \"bank_churn\"\n",
    "additional_parameters = sisense_conn.load_additional_parameters(cube_name, table_name=\"batch_prediction\")\n",
    "sisense_conn.set_parameters(cube_name=cube_name, additional_parameters=additional_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML Batch Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install swig -y\n",
    "!pip install Cython\n",
    "!pip install scikit-learn==0.24.0,
    "!pip install auto-sklearn\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imorting the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the datasets from elasticube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Data using Input Parameter\n",
    "table = sisense_conn.add_param[\"Dataset\"][\"table\"]\n",
    "drop_column = sisense_conn.add_param[\"Drop Feature\"]\n",
    "# Define Sql Statement\n",
    "logical_sql1 = (f'SELECT * from \"{table}\"')\n",
    "print(\"SQL Statement:\\n\" + logical_sql1)\n",
    "\n",
    "# Execute the SQL Statement\n",
    "logical_sql_res1 = sisense_conn.get_logical_sql(query=logical_sql1, \n",
    "                                               cube_name=cube_name,  # passed to notebook from build / Test Cell\n",
    "                                               count=None)  # limit the rows fetched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Known null representations needed since row from EC having null or empty values are represted as below in the list/array\n",
    "null_representations = ['N\\\\A', 'NA', 'N/A', '']\n",
    "column_names = logical_sql_res1['headers']\n",
    "values = logical_sql_res1['values']\n",
    "# Function to replace null representations\n",
    "def replace_nulls(data, null_representations):\n",
    "    return [[np.nan if v in null_representations else v for v in row] for row in data]\n",
    "\n",
    "# Clean the data\n",
    "values_cleaned = replace_nulls(values, null_representations)\n",
    "# Get Data\n",
    "df = pd.DataFrame(values_cleaned, columns = column_names)\n",
    "if drop_column != '0':\n",
    "    # Split the drop_column string into a list of column names\n",
    "    columns_to_drop = drop_column.split(',')\n",
    "    # Drop the specified columns\n",
    "    df = df.drop(columns_to_drop, axis=1)\n",
    "df.dtypes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model Name using Input Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_table = sisense_conn.add_param[\"Model\"][\"table\"]\n",
    "model_column = sisense_conn.add_param[\"Model\"][\"column\"]\n",
    "model_path = sisense_conn.add_param[\"Path\"][\"column\"]\n",
    "# Define Sql Statement\n",
    "logical_sql1 = (f'SELECT {model_column}, {model_path} from \"{model_table}\"')\n",
    "print(\"SQL Statement:\\n\" + logical_sql1)\n",
    "# Execute the SQL Statement\n",
    "logical_sql_res1 = sisense_conn.get_logical_sql(query=logical_sql1, \n",
    "                                               cube_name=cube_name,  # passed to notebook from build / Test Cell\n",
    "                                               count=None)  # limit the rows fetched\n",
    "# check for errors in logical sql execution \n",
    "# if \"error\" in logical_sql_res1:\n",
    "#     raise err.CustomCodeException(*err.ERROR_IN_LOGICAL_SQL, description=logical_sql_res1.get(\"details\"))\n",
    "column_name = logical_sql_res1['headers']\n",
    "values = logical_sql_res1['values']\n",
    "column_name = logical_sql_res1['headers']\n",
    "values = logical_sql_res1['values']\n",
    "# Get Data\n",
    "model = pd.DataFrame(values, columns = column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Numeric and Categorical column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Numeric Columns\n",
    "folder_path = model.iloc[0].path\n",
    "numeric_columns = joblib.load(folder_path + \"numeric_column_list\")\n",
    "numeric_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Categorical Columns\n",
    "categorical_columns = joblib.load(folder_path + \"categorical_column_list\")\n",
    "categorical_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load Date Columns\n",
    "date_columns = joblib.load(folder_path + \"date_column_list\")\n",
    "date_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Data Types of new data to modeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the list and convert each column in df to datetime, then to year and month\n",
    "for col in date_columns:\n",
    "    # Check if the column exists in the DataFrame\n",
    "    if col in df.columns:\n",
    "        # Convert to datetime if not already done\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        \n",
    "        # Create new columns for year and month\n",
    "        df[col + '_YEAR'] = df[col].dt.year.astype(str)\n",
    "        df[col + '_MONTH'] = df[col].dt.month.astype(str)\n",
    "        \n",
    "        # Drop the original date column\n",
    "        df.drop(columns=[col], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in categorical_columns:\n",
    "    df[column] = df[column].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in numeric_columns:\n",
    "    # Attempt to convert to integer, but handle cases where the value is a float\n",
    "    try:\n",
    "        # Try converting the column to float first\n",
    "        converted = pd.to_numeric(df[column])\n",
    "        if all(converted % 1 == 0):  # Check if all values are integers\n",
    "            df[column] = converted.astype(int)\n",
    "        else:\n",
    "            df[column] = converted\n",
    "    except ValueError:\n",
    "        # If conversion to float fails, keep the column as is\n",
    "        pass\n",
    "else:\n",
    "    df[column] = df[column].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Feature Data Order to feed transfrom pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load feature data column order\n",
    "feature_columns = joblib.load(folder_path + \"feature_column_order\")\n",
    "feature_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Change column order of new data\n",
    "df = df[feature_columns]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Transformer Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading pipeline\n",
    "transformer_pipeline = joblib.load(folder_path + \"transformer_pipeline\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_features = transformer_pipeline.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Model\n",
    "trained_model = joblib.load(folder_path + f'{model.iloc[0].model_name}')\n",
    "#trained_model = joblib.load(folder_path + 'automl_classifier_2023-11-03 20:23:25.863716')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.leaderboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = trained_model.predict(encoded_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Prediction\"] = pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Output based on Use Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"Prediction\"] = np.where(df[\"Prediction\"]==0, 'Retained',\n",
    "#                             np.where(df[\"Prediction\"]==1, 'On-Time')))\n",
    "df[\"Prediction\"] = np.where(df[\"Prediction\"] == 0, 'Retained',\n",
    "                            np.where(df[\"Prediction\"] == 1, 'Churned', df[\"Prediction\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
